{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13-uRkWB3c0fWmsl1_bPc8w4VjHEy77LB",
      "authorship_tag": "ABX9TyPo/KNiCCEW6e8sZRTuVlv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevenbowler/EmployeeSurvey/blob/main/UploadPDFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload PDFs to HuggingFace Private Dataset\n",
        "Steven Bowler\n",
        "\n",
        "How to Upload\n",
        "\n",
        "1. Prepare Your Repo: Create a new dataset repository on huggingface.co (e.g., via the web UI or CLI: huggingface-cli repo create your-username/pdf-dataset --type dataset).\n",
        "2. Track PDFs with LFS: In your local repo, run git lfs track \"*.pdf\" to ensure large PDFs are stored efficiently. Add this to your .gitattributes file.\n",
        "3. Upload Options:\n",
        "  a. Via Git/CLI (for smaller batches): Use git add . and git commit -m \"Add PDFs\", then git push. For large uploads, enable HF Transfer for speed: pip install huggingface_hub[hf_transfer] and set  HF_HUB_ENABLE_HF_TRANSFER=1.\n",
        "  b. Programmatic (Recommended for 1000 Files): Use the huggingface_hub library to upload folders in chunks:"
      ],
      "metadata": {
        "id": "3wW0tEWcOgSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "G_J3jB_atatQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Configuration\n",
        "# Replace with your values\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')  # Your HF token for private repo\n",
        "REPO_ID = userdata.get('REPO_ID')  # e.g., \"user/private-survey-pdfs\"\n",
        "XAI_API_KEY = userdata.get('XAI_API_KEY')  # Get from https://console.x.ai/\n",
        "MODEL = \"grok-4\"  # Or \"grok-4-fast-reasoning\" for cheaper/faster\n",
        "NUM_QUESTIONS = 25\n",
        "PDF_DIR = userdata.get('PDF_FOLDER_PATH')"
      ],
      "metadata": {
        "id": "Y65DBHF9tcx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEeum3T_OZQO"
      },
      "outputs": [],
      "source": [
        "# Confirmed: fastest, easiest way to upload 1,500 multi-page pdfs to huggingface\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=PDF_DIR,\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This handles LFS automatically and is efficient for many files.\n",
        "If total size is huge, compress PDFs into tarballs (e.g., 100 per file) to reduce the file count.\n",
        "\n",
        "\n",
        "Dataset Structure: Organize PDFs in a folder like /data/pdfs/ and add a README.md or Dataset Card describing the content (e.g., sources, licenses) for better discoverability."
      ],
      "metadata": {
        "id": "8IXU0udpO9m3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab notebook code that will:\n",
        "\n",
        "1. Mount Google Drive (where your 1000 PDFs are stored)\n",
        "2. Authenticate with Hugging Face\n",
        "3. Create a new dataset repository (if needed)\n",
        "4. Upload the folder of 1000 PDFs using  upload_folder() with HF Transfer for speed\n",
        "5. Add a basic README.md Dataset Card"
      ],
      "metadata": {
        "id": "Z3BslbkqPXex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 1: Install required packages ===\n",
        "!pip install -q huggingface_hub[hf_transfer] PyPDF2\n",
        "\n",
        "# === STEP 2: Mount Google Drive (where your PDFs are stored) ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === STEP 3: Login to Hugging Face ===\n",
        "from huggingface_hub import login\n",
        "login()  # This will prompt you to enter your HF token\n",
        "\n",
        "# === STEP 4: Set your paths and repo details ===\n",
        "import os\n",
        "\n",
        "# UPDATE THESE PATHS\n",
        "PDF_FOLDER_PATH = \"/content/drive/MyDrive/pdfs\"  # Folder with your 1000 PDFs\n",
        "REPO_ID = \"your-username/pdf-dataset\"            # Change to your username and dataset name\n",
        "REPO_TYPE = \"dataset\"\n",
        "\n",
        "# Optional: Create a Dataset Card (README.md)\n",
        "README_CONTENT = \"\"\"\n",
        "# PDF Dataset - 1000 Documents\n",
        "\n",
        "This dataset contains 1000 PDF files for research, NLP, or document analysis.\n",
        "\n",
        "## Structure\n",
        "- `pdfs/` - All 1000 PDF files\n",
        "\n",
        "## Usage\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"your-username/pdf-dataset\")"
      ],
      "metadata": {
        "id": "2mlDsMhzPNmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=== STEP 5: Upload the folder using HF Transfer (fast & reliable) ===\n",
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "#Enable HF Transfer for fast uploads\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "wejzcZt2SMMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create repo if it doesn't exist\n",
        "try:\n",
        "api.create_repo(repo_id=REPO_ID, repo_type=REPO_TYPE, private=False)\n",
        "print(f\"Created new dataset repo: {REPO_ID}\")\n",
        "except Exception as e:\n",
        "print(f\"Repo likely already exists: {e}\")"
      ],
      "metadata": {
        "id": "f9snRq3sSXRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload the PDFs folder\n",
        "print(\"Starting upload of 1000 PDFs... This may take a while.\")\n",
        "api.upload_folder(\n",
        "folder_path=PDF_FOLDER_PATH,\n",
        "repo_id=REPO_ID,\n",
        "repo_type=REPO_TYPE,\n",
        "path_in_repo=\"pdfs\",  # Uploads to /pdfs in the repo\n",
        "allow_patterns=[\".pdf\"],  # Only upload PDFs\n",
        "ignore_patterns=[\".txt\", \"*.csv\"],  # Optional: ignore non-PDF\n",
        ")"
      ],
      "metadata": {
        "id": "ZOisruSLS4qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload README.md\n",
        "from huggingface_hub import upload_file\n",
        "import tempfile\n",
        "with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:\n",
        "f.write(README_CONTENT)\n",
        "readme_path = f.name\n",
        "upload_file(\n",
        "path_or_fileobj=readme_path,\n",
        "path_in_repo=\"README.md\",\n",
        "repo_id=REPO_ID,\n",
        "repo_type=REPO_TYPE\n",
        ")\n",
        "print(f\"Successfully uploaded 1000 PDFs to https://huggingface.co/datasets/{REPO_ID}\")\n",
        "print(\"View your dataset card and files on Hugging Face!\")"
      ],
      "metadata": {
        "id": "gy7xNSyfSakF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Readme.md Text\n",
        "---\n",
        "\n",
        "### Instructions Before Running\n",
        "\n",
        "1. **Upload your 1000 PDFs** to a folder in Google Drive (e.g., `MyDrive/pdfs`)\n",
        "2. **Get a Hugging Face token**:\n",
        "   - Go to: https://huggingface.co/settings/tokens\n",
        "   - Create a **new token** with `write` access\n",
        "3. **Update these lines**:\n",
        "   ```python\n",
        "   PDF_FOLDER_PATH = \"/content/drive/MyDrive/pdfs\"  # Your folder\n",
        "   REPO_ID = \"your-username/pdf-dataset\"            # e.g., \"johnsmith/research-pdfs\""
      ],
      "metadata": {
        "id": "hH_rUfzBSe4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following 3 cells are a one off test of Grok Interface"
      ],
      "metadata": {
        "id": "QVfZz3xfnvVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#following 3 code cells test Grok Interface\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "5lHKxSgym53A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Configuration\n",
        "# Replace with your values\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')  # Your HF token for private repo\n",
        "REPO_ID = userdata.get('REPO_ID')  # e.g., \"user/private-survey-pdfs\"\n",
        "XAI_API_KEY = userdata.get('XAI_API_KEY')  # Get from https://console.x.ai/\n",
        "MODEL = \"grok-4\"  # Or \"grok-4-fast-reasoning\" for cheaper/faster\n",
        "NUM_QUESTIONS = 25\n",
        "PDF_DIR = userdata.get('PDF_FOLDER_PATH')"
      ],
      "metadata": {
        "id": "Gg70311_m2Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test XAI_API_KEY\n",
        "# Use f-strings to correctly format the curl command with the API key\n",
        "import os\n",
        "\n",
        "curl_command = f\"\"\"curl https://api.x.ai/v1/chat/completions \\\\\n",
        "    -H \"Content-Type: application/json\" \\\\\n",
        "    -H \"Authorization: Bearer {XAI_API_KEY}\" \\\\\n",
        "    -d '{{\n",
        "      \"messages\": [\n",
        "        {{\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a test assistant.\"\n",
        "        }},\n",
        "        {{\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"Testing. Just say hi and hello world and nothing else.\"\n",
        "        }}\n",
        "      ],\n",
        "      \"model\": \"grok-4-latest\",\n",
        "      \"stream\": false,\n",
        "      \"temperature\": 0\n",
        "    }}'\"\"\"\n",
        "\n",
        "# Execute the curl command using os.system or subprocess.run\n",
        "# os.system is simpler for this case, but subprocess.run is generally preferred for more control\n",
        "os.system(curl_command)"
      ],
      "metadata": {
        "id": "muryMnkFl_MA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}