{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IEyMpK3lof2iSWj_ZQ1GcZOBPbg-0IUc",
      "authorship_tag": "ABX9TyMK9m8HHZ3Yssdb+XqDsmA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevenbowler/EmployeeSurvey/blob/main/EmployeeSurvey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Employee Survey Processor Using Grok4\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Run cells sequentially.\n",
        "Tokens: Replace HF_TOKEN, REPO_ID, XAI_API_KEY.\n",
        "HF Repo: Ensure your private repo contains only PDFs (or patterns match).\n",
        "Cost: ~1000 API calls for extraction + 1 summary. Use grok-4-fast-reasoning to save ~90%.\n",
        "Multi-page PDFs: Modify pdf_to_base64 to send multiple images (append more image_url).\n",
        "Errors: Check console for failed parses; manually review if needed.\n",
        "Scale: For 1000 PDFs, ~1-2 hours runtime.\n",
        "\n",
        "This code automatically:\n",
        "\n",
        "Downloads 1000+ PDFs.\n",
        "Uses Grok-4 Vision to parse scanned sheets.\n",
        "Computes exact % per choice/question.\n",
        "Generates AI expert summary.\n",
        "Plots interactive charts + CSV export."
      ],
      "metadata": {
        "id": "cWdFjf4nMPVi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taEAkWdWLly4"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!apt-get update -qq\n",
        "!apt-get install -y tesseract-ocr poppler-utils -qq\n",
        "!pip install -q pdf2image huggingface_hub openai pillow pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports\n",
        "import os\n",
        "import glob\n",
        "import base64\n",
        "import json\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "from pdf2image import convert_from_path\n",
        "from openai import OpenAI\n",
        "from huggingface_hub import snapshot_download\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "giNQE8fhLz4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Configuration\n",
        "# Replace with your values\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')  # Your HF token for private repo\n",
        "REPO_ID = userdata.get('REPO_ID')  # e.g., \"user/private-survey-pdfs\"\n",
        "XAI_API_KEY = userdata.get('XAI_API_KEY')  # Get from https://console.x.ai/\n",
        "MODEL = \"grok-4\"  # Or \"grok-4-fast-reasoning\" for cheaper/faster\n",
        "NUM_QUESTIONS = 25\n",
        "PDF_DIR = \"/content/pdfs\""
      ],
      "metadata": {
        "id": "o8MQhlSQL3Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Download PDFs from private HF repo\n",
        "os.makedirs(PDF_DIR, exist_ok=True)\n",
        "pdf_dir = snapshot_download(\n",
        "    repo_id=REPO_ID,\n",
        "    token=HF_TOKEN,\n",
        "    local_dir=PDF_DIR,\n",
        "    repo_type=\"dataset\",  # Change to \"model\" if it's a model repo\n",
        "    allow_patterns=[\"*.pdf\"],\n",
        "    ignore_patterns=[\"*.json\", \"*.md\", \"*config*\"]\n",
        ")\n",
        "print(f\"Downloaded PDFs to {pdf_dir}\")\n",
        "\n",
        "pdf_paths = sorted(glob.glob(f\"{pdf_dir}/**/*.pdf\", recursive=True))\n",
        "print(f\"Found {len(pdf_paths)} PDF files\")"
      ],
      "metadata": {
        "id": "zI7NxdB_L47H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Initialize xAI client\n",
        "client = OpenAI(\n",
        "    api_key=XAI_API_KEY,\n",
        "    base_url=\"https://api.x.ai/v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "i_NSoGitL7xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Extract answers using Grok-4 Vision\n",
        "def pdf_to_base64(pdf_path, page_num=0):\n",
        "    \"\"\"Convert first page of PDF to base64 PNG\"\"\"\n",
        "    images = convert_from_path(pdf_path, first_page=page_num+1, last_page=page_num+1)\n",
        "    if not images:\n",
        "        return None\n",
        "    img = images[0]\n",
        "    buffered = BytesIO()\n",
        "    img.save(buffered, format=\"PNG\")\n",
        "    return base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "def extract_answers(pdf_path):\n",
        "    \"\"\"Use Grok-4 to extract answers from PDF image\"\"\"\n",
        "    b64 = pdf_to_base64(pdf_path)\n",
        "    if not b64:\n",
        "        print(f\"Failed to process {pdf_path}\")\n",
        "        return None\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": \"\"\"You are an expert at reading multiple-choice answer sheets.\n",
        "This is a scanned/image answer sheet for a 25-question survey.\n",
        "Each question has choices A, B, C, D (possibly E).\n",
        "Identify the marked/bubbled/chosen answer for each question 1-25.\n",
        "Return ONLY a valid JSON array of 25 uppercase letters, e.g., [\"A\", \"B\", \"C\", ...].\n",
        "If unclear, use \"X\". Do not add any other text.\"\"\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"}\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=100,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content.strip()\n",
        "    try:\n",
        "        answers = json.loads(content)\n",
        "        if isinstance(answers, list) and len(answers) == NUM_QUESTIONS:\n",
        "            return [a.upper() for a in answers]\n",
        "        else:\n",
        "            print(f\"Invalid format from {pdf_path}: {content}\")\n",
        "            return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"JSON parse error from {pdf_path}: {content}\")\n",
        "        return None\n",
        "\n",
        "# Process all PDFs\n",
        "all_answers = []\n",
        "for i, pdf_path in enumerate(pdf_paths):\n",
        "    print(f\"Processing {i+1}/{len(pdf_paths)}: {Path(pdf_path).name}\")\n",
        "    answers = extract_answers(pdf_path)\n",
        "    if answers:\n",
        "        all_answers.append(answers)\n",
        "\n",
        "print(f\"Successfully processed {len(all_answers)} / {len(pdf_paths)} PDFs\")"
      ],
      "metadata": {
        "id": "-GwXi4PhL8-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Aggregate percentages\n",
        "question_stats = defaultdict(lambda: defaultdict(int))\n",
        "for answers in all_answers:\n",
        "    for q_idx, choice in enumerate(answers, 1):\n",
        "        question_stats[q_idx][choice] += 1\n",
        "\n",
        "# Compute percentages\n",
        "percentages = {}\n",
        "total_responses = len(all_answers)\n",
        "for q, counts in question_stats.items():\n",
        "    total_q = sum(counts.values())\n",
        "    percs = {choice: (count / total_q * 100) for choice, count in counts.items()}\n",
        "    percentages[q] = dict(sorted(percs.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "print(f\"Total responses: {total_responses}\")\n",
        "print(\"Percentages per question:\")\n",
        "for q, percs in percentages.items():\n",
        "    print(f\"Q{q}: {percs}\")"
      ],
      "metadata": {
        "id": "iipspCykMCst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Generate summary report with Grok-4\n",
        "summary_prompt = f\"\"\"You are a data analyst expert.\n",
        "Here is the survey results from {total_responses} respondents for 25 multiple-choice questions.\n",
        "\n",
        "Percentages (most popular first):\n",
        "{json.dumps(percentages, indent=2)}\n",
        "\n",
        "Generate a professional summary report:\n",
        "1. Overall insights: most consistent questions, popular choices.\n",
        "2. Table of top answer % per question.\n",
        "3. Any patterns or anomalies.\n",
        "4. Visual description (as if charts).\n",
        "5. Recommendations if it were a survey.\n",
        "\n",
        "Use markdown with tables. Be concise yet insightful.\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
        "    max_tokens=2000,\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "print(\"### Grok-4 Expert Summary Report\")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "PaM0dskvMG6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Local visualization (bar charts for each question)\n",
        "fig, axes = plt.subplots(5, 5, figsize=(20, 20))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (q, percs) in enumerate(percentages.items()):\n",
        "    choices = list(percs.keys())\n",
        "    vals = list(percs.values())\n",
        "    axes[idx].bar(choices, vals, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
        "    axes[idx].set_title(f'Q{q} (%)')\n",
        "    axes[idx].set_ylim(0, 100)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Export to CSV\n",
        "df = pd.DataFrame(percentages).T.fillna(0)\n",
        "df.to_csv('/content/survey_percentages.csv')\n",
        "print(\"Exported to /content/survey_percentages.csv\")"
      ],
      "metadata": {
        "id": "RW7Lg-RtMKNe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}